{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from feature_fusion import convert_xlsx_to_csv\n",
    "\n",
    "def merge_data_by(onwhat,name,qtype):\n",
    "    data1 =  pd.read_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_drug.csv')\n",
    "    data1 = data1.drop('Index', axis=1)\n",
    "    data2 = pd.read_csv('/data/zyf/HyperGCN-DTI/data/Yamanishi/Formatted_Drug_Disease_Data.csv')\n",
    "    merged_data = data1.merge(data2,on=onwhat,how='left')\n",
    "    merged_data = merged_data.drop('level_1',axis=1)\n",
    "    merged_data.to_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_{qtype}.csv',index = False)\n",
    "for name in ['Es','GPCRs','ICs','NRs']:\n",
    "        convert_xlsx_to_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_drug.xlsx',f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_drug.csv','KEGG Drug ID')\n",
    "        convert_xlsx_to_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_protein.xlsx',f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_protein.csv','pro_id')\n",
    "        merge_data_by('KEGG Drug ID',name,'drug_disease')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nteraction_matrix(name):\n",
    "    # 生成药物与疾病的相互作用矩阵\n",
    "    drug_disease_df = pd.read_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_drug_disease.csv')\n",
    "    interaction_matrix = pd.crosstab(drug_disease_df['KEGG Drug ID'], drug_disease_df['Efficacy and Disease'])\n",
    "    #interaction_matrix = interaction_matrix.drop(columns=['KEGG Drug ID']) \n",
    "    # 将矩阵保存到新的CSV文件中\n",
    "    interaction_matrix_file = f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_mat_drug_disease.txt'\n",
    "    interaction_matrix.to_csv(interaction_matrix_file, sep='\\t',index=False,header=False)\n",
    "\n",
    "    # 返回生成的文件路径\n",
    "    interaction_matrix_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['Es','GPCRs','ICs','NRs']:\n",
    "    generate_nteraction_matrix(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整理出的疾病种类:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 目标网页 URL\n",
    "url = \"https://www.genome.jp/kegg/catalog/pathway_dd.html\"\n",
    "\n",
    "# 发起 HTTP 请求获取网页内容\n",
    "response = requests.get(url)\n",
    "\n",
    "# 检查请求是否成功\n",
    "if response.status_code == 200:\n",
    "    # 解析网页内容\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 查找疾病相关的表格或列表\n",
    "    # 假设每个疾病链接在 <a> 标签中并且包含在特定的部分内\n",
    "    disease_list = []\n",
    "    \n",
    "    # 在网页中找到所有的链接或疾病名称\n",
    "    for link in soup.find_all('0'):\n",
    "        # 检查链接文字是否和疾病相关\n",
    "        text = link.text.strip()\n",
    "        print(text)\n",
    "        if text and \"disease\" in text.lower():  # 这里假设疾病相关的链接包含 'disease'\n",
    "            disease_list.append(text)\n",
    "    \n",
    "    # 输出整理的疾病列表\n",
    "    print(\"整理出的疾病种类:\")\n",
    "    for disease in disease_list:\n",
    "        print(disease)\n",
    "else:\n",
    "    print(f\"无法访问该网页，状态码: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract disease-related information\n",
    "def extract_disease_info(kegg_data):\n",
    "    lines = kegg_data.split('\\n')\n",
    "    #print(lines)\n",
    "    disease_list  = pd.read_csv('/data/zyf/HyperGCN-DTI/data/Yamanishi/diseases_list.txt', header=None, names=[\"Disease\"])\n",
    "    for disease in disease_list['Disease']:\n",
    "        disease_section = [line for line in lines if disease in line]\n",
    "        #print (disease_section)\n",
    "    return disease_section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Fetching disease information for protein ID: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(protein_id[\u001b[38;5;241m4\u001b[39m:])\n\u001b[1;32m     89\u001b[0m extractor \u001b[38;5;241m=\u001b[39m KEGGProteinDiseaseExtractor(protein_id[\u001b[38;5;241m4\u001b[39m:])\n\u001b[0;32m---> 90\u001b[0m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_all_diseases\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m extractor\u001b[38;5;241m.\u001b[39msave_to_csv()\n",
      "Cell \u001b[0;32mIn[53], line 70\u001b[0m, in \u001b[0;36mKEGGProteinDiseaseExtractor.fetch_all_diseases\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotein_ids:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching disease information for protein ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m     diseases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_disease_pathways\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisease information for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(diseases)\n",
      "Cell \u001b[0;32mIn[53], line 19\u001b[0m, in \u001b[0;36mKEGGProteinDiseaseExtractor.get_disease_pathways\u001b[0;34m(self, protein_id)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mFetch KEGG disease pathways for a given protein ID.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kegg.jp/kegg-bin/search_brite?option=-a&search_string=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotein_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&prefix=hsa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     22\u001b[0m     brite_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/data/zyf/anaconda3/envs/my_env/lib/python3.8/http/client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/data/zyf/anaconda3/envs/my_env/lib/python3.8/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/data/zyf/anaconda3/envs/my_env/lib/python3.8/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/zyf/anaconda3/envs/my_env/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/data/zyf/anaconda3/envs/my_env/lib/python3.8/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/data/zyf/anaconda3/envs/my_env/lib/python3.8/ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "class KEGGProteinDiseaseExtractor:\n",
    "    def __init__(self, protein_ids):\n",
    "        \"\"\"\n",
    "        Initialize the KEGGProteinDiseaseExtractor with a list of protein IDs.\n",
    "        \"\"\"\n",
    "        self.protein_ids = protein_ids\n",
    "        self.results = []\n",
    "\n",
    "    def get_disease_pathways(self, protein_id):\n",
    "        \"\"\"\n",
    "        Fetch KEGG disease pathways for a given protein ID.\n",
    "        \"\"\"\n",
    "        url = f\"https://www.kegg.jp/kegg-bin/search_brite?option=-a&search_string={protein_id}&prefix=hsa\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            brite_data = response.text\n",
    "            # Extract Human Disease data\n",
    "            human_disease_data = self.extract_human_disease_data(brite_data)\n",
    "            cleaned_data = self.clean_html(human_disease_data)\n",
    "            return cleaned_data\n",
    "        else:\n",
    "            return f\"Error: Could not retrieve data for {protein_id}\"\n",
    "\n",
    "    def extract_human_disease_data(self, brite_data):\n",
    "        \"\"\"\n",
    "        Extract Human Disease information from KEGG BRITE data using regex.\n",
    "        \"\"\"\n",
    "        # Regex to match the Human Diseases section and its sub-hierarchy\n",
    "        human_disease_pattern = re.compile(r\"(09160 Human Diseases.*?)(09180 Brite Hierarchies|$)\", re.S)\n",
    "        \n",
    "        match = human_disease_pattern.search(brite_data)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return \"No Human Disease data found\"\n",
    "\n",
    "    def clean_html(self, data):\n",
    "        \"\"\"\n",
    "        Clean HTML tags and unnecessary characters from the disease data.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        cleaned_text = soup.get_text()\n",
    "\n",
    "        # Clean up extra spaces and HTML entities\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        cleaned_text = re.sub(r'&nbsp;', ' ', cleaned_text)\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "\n",
    "        # Only keep text before [PATH:*] pattern\n",
    "        cleaned_text = re.split(r'\\[PATH:', cleaned_text)[0]\n",
    "\n",
    "        # Remove \"09160 Human Diseases\" and codes, and format the result\n",
    "        cleaned_text = cleaned_text.replace(\"09160 Human Diseases\", \"\").strip()\n",
    "        cleaned_text = re.sub(r'\\d{5} ', '', cleaned_text)  # Remove 5-digit codes\n",
    "        cleaned_text = re.sub(r',+', ',', re.sub(r'\\s+,', ',', cleaned_text))  # Clean extra spaces and commas\n",
    "\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    def fetch_all_diseases(self):\n",
    "        \"\"\"\n",
    "        Fetch disease information for all protein IDs in the list and store the results.\n",
    "        \"\"\"\n",
    "        for pid in self.protein_ids:\n",
    "            pid = pid[4:]\n",
    "            print(f\"Fetching disease information for protein ID: {pid}\")\n",
    "            diseases = self.get_disease_pathways(pid)\n",
    "            print(f\"Disease information for {pid}:\")\n",
    "            print(diseases)\n",
    "            print(\"=\" * 50)  # Separator for better readability\n",
    "            self.results.append([pid, diseases])\n",
    "\n",
    "    def save_to_csv(self, filename='protein_disease_info.csv'):\n",
    "        \"\"\"\n",
    "        Save the results to a CSV file.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.results, columns=['Protein ID', 'Disease Information'])\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Data has been saved to {filename}\")\n",
    "for name in ['Es','GPCRs','ICs','NRs']:\n",
    "    protein_ids_list = pd.read_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_protein.csv')\n",
    "    # Example usage:\n",
    "    protein_ids = protein_ids_list['pro_id'] # Replace with actual protein IDs\n",
    "\n",
    "    extractor = KEGGProteinDiseaseExtractor(protein_ids)\n",
    "    extractor.fetch_all_diseases()\n",
    "    extractor.save_to_csv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html><head><title>Search BRITE</title>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/get_htext.css\">\n",
      "</head><body>\n",
      "<h2>Search BRITE hierarchies</h2><pre>\n",
      "KEGG Orthology (KO) [BR:<a href=\"/brite/hsa00001+160\">hsa00001</a>]\n",
      "&nbsp;&nbsp;09140 Cellular Processes\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09141 Transport and catabolism\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04144 Endocytosis [PATH:<a href=\"/pathway/hsa04144\">hsa04144</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "&nbsp;&nbsp;09150 Organismal Systems\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09155 Excretory system\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04961 Endocrine and other factor-regulated calcium reabsorption [PATH:<a href=\"/pathway/hsa04961\">hsa04961</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09156 Nervous system\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04721 Synaptic vesicle cycle [PATH:<a href=\"/pathway/hsa04721\">hsa04721</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "&nbsp;&nbsp;09160 Human Diseases\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09164 Neurodegenerative disease\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;05016 Huntington disease [PATH:<a href=\"/pathway/hsa05016\">hsa05016</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "&nbsp;&nbsp;09180 Brite Hierarchies\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09182 Protein families: genetic information processing\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04131 Membrane trafficking [BR:<a href=\"/brite/hsa04131+160\">hsa04131</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09183 Protein families: signaling and cellular processes\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04147 Exosome [BR:<a href=\"/brite/hsa04147+160\">hsa04147</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "\n",
      "Membrane trafficking [BR:<a href=\"/brite/hsa04131+160\">hsa04131</a>]\n",
      "&nbsp;&nbsp;Endocytosis\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;Clathrin-mediated endocytosis\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AP-2 complex\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha \n",
      "\n",
      "Exosome [BR:<a href=\"/brite/hsa04147+160\">hsa04147</a>]\n",
      "&nbsp;&nbsp;Exosomal proteins\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;Exosomal proteins of colorectal cancer cells\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha \n",
      "</pre>\n",
      "<hr><b>[ <a href=/kegg/brite.html>BRITE</a> | <a href=/kegg/kegg2.html>KEGG2</a> | <a href=/kegg/>KEGG</a> ]</b>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "<re.Match object; span=(1579, 3811), match='09160 Human Diseases\\n&nbsp;&nbsp;&nbsp;&nbsp;091>\n",
      "Disease information for 160:\n",
      "09160 Human Diseases\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09164 Neurodegenerative disease\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;05016 Huntington disease [PATH:<a href=\"/pathway/hsa05016\">hsa05016</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "&nbsp;&nbsp;09180 Brite Hierarchies\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09182 Protein families: genetic information processing\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04131 Membrane trafficking [BR:<a href=\"/brite/hsa04131+160\">hsa04131</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09183 Protein families: signaling and cellular processes\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04147 Exosome [BR:<a href=\"/brite/hsa04147+160\">hsa04147</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "\n",
      "Membrane trafficking [BR:<a href=\"/brite/hsa04131+160\">hsa04131</a>]\n",
      "&nbsp;&nbsp;Endocytosis\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;Clathrin-mediated endocytosis\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AP-2 complex\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha \n",
      "\n",
      "Exosome [BR:<a href=\"/brite/hsa04147+160\">hsa04147</a>]\n",
      "&nbsp;&nbsp;Exosomal proteins\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;Exosomal proteins of colorectal cancer cells\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha \n",
      "</pre>\n",
      "<hr><b>[ <a href=/kegg/brite.html>BRITE</a> | <a href=/kegg/kegg2.html>KEGG2</a> | <a href=/kegg/>KEGG</a> ]</b>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Example function to get KEGG disease pathways\n",
    "def get_disease_pathways(protein_id):\n",
    "    url = f\"https://www.kegg.jp/kegg-bin/search_brite?option=-a&search_string={protein_id}&prefix=hsa\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        brite_data = response.text\n",
    "        print(brite_data)\n",
    "        human_disease_data = extract_human_disease_data(brite_data)\n",
    "        return human_disease_data\n",
    "    else:\n",
    "        return f\"Error: Could not retrieve data for {protein_id}\"\n",
    "# 用正则表达式匹配Human Disease及其子层级\n",
    "def extract_human_disease_data(brite_data):\n",
    "    # 匹配 Human Diseases 部分及其子层级\n",
    "    human_disease_pattern = re.compile(r\"(09160 Human Diseases.*?)(\\n  09180|$)\", re.S)\n",
    "    \n",
    "    # 提取 Human Diseases 部分\n",
    "    match = human_disease_pattern.search(brite_data)\n",
    "    print(match)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "# Replace these IDs with your actual protein IDs\n",
    "protein_ids = ['160']  # Example IDs\n",
    "\n",
    "# Fetch and display disease information for each protein\n",
    "for pid in protein_ids:\n",
    "    diseases = get_disease_pathways(pid)\n",
    "    print(f\"Disease information for {pid}:\")\n",
    "    print(diseases)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09160 Human Diseases\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09164 Neurodegenerative disease\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;05016 Huntington disease [PATH:<a href=\"/pathway/hsa05016\">hsa05016</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "&nbsp;&nbsp;09180 Brite Hierarchies\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09182 Protein families: genetic information processing\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04131 Membrane trafficking [BR:<a href=\"/brite/hsa04131+160\">hsa04131</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;09183 Protein families: signaling and cellular processes\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;04147 Exosome [BR:<a href=\"/brite/hsa04147+160\">hsa04147</a>]\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha\n",
      "\n",
      "Membrane trafficking [BR:<a href=\"/brite/hsa04131+160\">hsa04131</a>]\n",
      "&nbsp;&nbsp;Endocytosis\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;Clathrin-mediated endocytosis\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AP-2 complex\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha \n",
      "\n",
      "Exosome [BR:<a href=\"/brite/hsa04147+160\">hsa04147</a>]\n",
      "&nbsp;&nbsp;Exosomal proteins\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;Exosomal proteins of colorectal cancer cells\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"/entry/hsa:160\"><font color=red>160</font></a> AP2A1; adaptor related protein complex 2 subunit alpha 1\t<a href=\"/entry/K11824\">K11824</a> AP2A; AP-2 complex subunit alpha \n",
      "</pre>\n",
      "<hr><b>[ <a href=/kegg/brite.html>BRITE</a> | <a href=/kegg/kegg2.html>KEGG2</a> | <a href=/kegg/>KEGG</a> ]</b>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Disease\n",
      "0                       Pathways in cancer\n",
      "1  Transcriptional misregulation in cancer\n",
      "2                      MicroRNAs in cancer\n",
      "3                  Proteoglycans in cancer\n",
      "4    Chemical carcinogenesis - DNA adducts\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/data/zyf/HyperGCN-DTI/data/Yamanishi/diseases_list.txt', header=None, names=[\"Disease\"])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     0.97582\n",
      "AUROC        0.99760\n",
      "AUPr         0.99756\n",
      "Precision    0.96696\n",
      "recall       0.98544\n",
      "f1           0.97606\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data =pd.read_csv('/data/zyf/HyperGCN-DTI/results/Luo_CV_resluts.csv')\n",
    "print(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_xlsx_to_csv(excel_file_path,csv_file_path,qtype):\n",
    "    df = pd.read_excel(excel_file_path, header=None)\n",
    "    df.columns = ['Index',qtype]\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "for name in ['Es','GPCRs','ICs','NRs']:\n",
    "    convert_xlsx_to_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_drug.xlsx',f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_drug_smiles.csv','KEGG Drug ID')\n",
    "    convert_xlsx_to_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_protein.xlsx',f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_protein_seq.csv','pro_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_data_by(onwhat,name,qtype):\n",
    "    data1 =  pd.read_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_{qtype}.csv')\n",
    "    print(data1.columns)\n",
    "    data1 = data1.drop('Index', axis=1)\n",
    "    data2 = pd.read_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{qtype}.csv')\n",
    "    merged_data = data1.merge(data2,on=onwhat,how='left')\n",
    "    merged_data.to_csv(f'/data/zyf/HyperGCN-DTI/data/Yamanishi/{name}/{name}_{qtype}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es\n",
      "Index(['Index', 'KEGG Drug ID'], dtype='object')\n",
      "Index(['Index', 'pro_id'], dtype='object')\n",
      "GPCRs\n",
      "Index(['Index', 'KEGG Drug ID'], dtype='object')\n",
      "Index(['Index', 'pro_id'], dtype='object')\n",
      "ICs\n",
      "Index(['Index', 'KEGG Drug ID'], dtype='object')\n",
      "Index(['Index', 'pro_id'], dtype='object')\n",
      "NRs\n",
      "Index(['Index', 'KEGG Drug ID'], dtype='object')\n",
      "Index(['Index', 'pro_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for name in ['Es','GPCRs','ICs','NRs']:\n",
    "    print(name)\n",
    "    merge_data_by('KEGG Drug ID',name,'drug_smiles')\n",
    "    merge_data_by('pro_id',name,'protein_seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import torch\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import warnings\n",
    "from Structe_DPP_HyperGraph import HyGraph_Matrix_DPP_Structure\n",
    "from hypergraph_utils import generate_G_from_H\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "from model import *\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from hypergraph_utils import generate_G_from_H\n",
    "from hypergraph_utils import construct_H_with_KNN\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "from Structe_DPP_HyperGraph import HyGraph_Matrix_DPP_Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#config['device'] = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "args = setup(default_configure, seed)\n",
    "in_size = 512\n",
    "hidden_size = 256\n",
    "out_size = 128\n",
    "dropout = 0.5\n",
    "lr = 0.0003\n",
    "weight_decay = 1e-10\n",
    "epochs = 800\n",
    "reg_loss_co = 0.0002\n",
    "fold = 0\n",
    "dir = \"/data/zyf/HyperGCN-DTI/modelSave\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([2220, 128])\n",
      "torch.Size([2220, 2220])\n",
      "3 torch.Size([2220, 128])\n",
      "torch.Size([708, 128]) torch.Size([1512, 128])\n",
      "torch.Size([3846, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3076, 2]) torch.Size([708, 128]) torch.Size([1512, 128])\n",
      "tensor(0.4880, device='cuda:0', dtype=torch.float64) tensor(0.8081, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function AddmmBackward returned an invalid gradient at index 2 - got [320, 512] but expected shape compatible with [1280, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 125\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLuo\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, aver Acc is:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(all_acc)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(all_acc)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,  Aver roc is:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(all_roc)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(all_roc)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAver Pr is:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(all_pr)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(all_pr)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ,Aver f1 is:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(all_f1)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(all_f1)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    124\u001b[0m train_indeces, test_indeces \u001b[38;5;241m=\u001b[39m get_cross(dtidata)\n\u001b[0;32m--> 125\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_indeces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indeces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 106\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(tr, te, seed)\u001b[0m\n\u001b[1;32m    104\u001b[0m best_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 106\u001b[0m     loss, train_acc, task1_roc, acc, task1_roc1, task1_pr, task1_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m acc \u001b[38;5;241m>\u001b[39m best_acc:\n\u001b[1;32m    108\u001b[0m         best_acc \u001b[38;5;241m=\u001b[39m acc\n",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optim, train_index, test_index, epoch, fold)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_acc,loss)\n\u001b[1;32m     48\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 49\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m te_acc, te_task1_roc1, te_task1_pr, te_task1_f1 \u001b[38;5;241m=\u001b[39m main_test(model, d, p, test_index, epoch, fold)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function AddmmBackward returned an invalid gradient at index 2 - got [320, 512] but expected shape compatible with [1280, 512]"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# for name in [\"Luo\",\"Es\",\"GPCRs\",\"ICs\",\"NRs\", \"Zheng\"]:\n",
    "# dataName: Luo Es GPCRs ICs NRs Zheng\n",
    "node_num, drug_protein, protein_drug, dtidata = load_dataset_from_name(\"Luo\")\n",
    "#[708, 1512] torch.Size([708, 1512]) torch.Size([1512, 708]) (3846, 3)\n",
    "drug_disease,protein_disease = load_disease_info('/data/zyf/HyperGCN-DTI/data/Luo/','mat_drug_disease.txt', 'mat_drug_disease.txt',\"Luo\")\n",
    "\n",
    "drug_protein_eye = torch.cat((drug_protein, torch.eye(node_num[0])), dim=1)\n",
    "protein_drug_eye = torch.cat((protein_drug, torch.eye(node_num[1])), dim=1)\n",
    "#hd = torch.randn((node_num[0], node_num[0]))\n",
    "#hp = torch.randn((node_num[1], node_num[1]))\n",
    "#替换为LLM特征\n",
    "dti_label = torch.tensor(dtidata[:, 2:3]).to(config['device'])\n",
    "hd = pd.read_csv('/data/zyf/HyperGCN-DTI/data/Luo/drug_smiles.csv')\n",
    "hp = pd.read_csv('/data/zyf/HyperGCN-DTI/data/Luo/protein_seq.csv')\n",
    "features_d = torch.tensor(hd.iloc[:,2:].values,dtype=torch.float32).to(config['device'])\n",
    "features_p = torch.tensor(hp.iloc[:,2:].values,dtype=torch.float32).to(config['device'])\n",
    "drug_disease_eye = torch.cat((drug_disease, torch.eye(node_num[0])), dim=1)\n",
    "protein_disease_eye = torch.cat((protein_disease, torch.eye(node_num[0])), dim=1)\n",
    "\n",
    "HyGraph_Drug = generate_G_from_H(drug_protein_eye).to(config['device'])\n",
    "HyGraph_protein = generate_G_from_H(protein_drug_eye).to(config['device'])\n",
    "HyGraph_Drug_disease = generate_G_from_H(drug_disease_eye).to(config['device'])\n",
    "HyGraph_protein_disease = generate_G_from_H(protein_disease_eye).to(config['device'])\n",
    "\n",
    "\n",
    "drug_protein = drug_protein.to(config['device'])\n",
    "protein_drug = protein_drug.to(config['device'])\n",
    "\n",
    "\n",
    "HyGraph_Structure_DPP = HyGraph_Matrix_DPP_Structure(dtidata, node_num[0], node_num[1])\n",
    "HyGraph_Structure_DPP = HyGraph_Structure_DPP.to(config['device'])\n",
    "\n",
    "data = dtidata\n",
    "label = dti_label\n",
    "\n",
    "\n",
    "def train(model, optim, train_index, test_index, epoch, fold):\n",
    "    model.train()\n",
    "    out, d, p = model(node_num, features_d, features_p, protein_drug, drug_protein, HyGraph_Drug, HyGraph_protein, train_index, data, HyGraph_Structure_DPP)\n",
    "    print(out.shape,d.shape,p.shape)\n",
    "    train_acc = (out.argmax(dim=1) == label[train_index].reshape(-1).long()).sum(dtype=float) / torch.tensor(len(train_index), dtype=float)\n",
    "    task1_roc = get_roc(out, label[train_index])\n",
    "    reg = get_L2reg(model.parameters())\n",
    "    loss = F.nll_loss(out, label[train_index].reshape(-1).long()) + reg_loss_co * reg\n",
    "    print(train_acc,loss)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    te_acc, te_task1_roc1, te_task1_pr, te_task1_f1 = main_test(model, d, p, test_index, epoch, fold)\n",
    "\n",
    "    return loss.item(), train_acc, task1_roc, te_acc, te_task1_roc1, te_task1_pr, te_task1_f1\n",
    "\n",
    "\n",
    "def main_test(model, d, p, test_index, epoch, fold):\n",
    "    model.eval()\n",
    "    out = model(node_num, features_d, features_p, protein_drug, drug_protein, HyGraph_Drug, HyGraph_protein, test_index, data, HyGraph_Structure_DPP, iftrain=False, d=d, p=p)\n",
    "\n",
    "    acc1 = (out.argmax(dim=1) == label[test_index].reshape(-1).long()).sum(dtype=float) / torch.tensor(len(test_index), dtype=float)\n",
    "    task_roc = get_roc(out, label[test_index])\n",
    "    task_pr = get_pr(out, label[test_index])\n",
    "    task_f1 = get_f1score(out, label[test_index])\n",
    "    # if epoch == 799:\n",
    "    #     f = open(f\"{fold}out.txt\",\"w\",encoding=\"utf-8\")\n",
    "    #     for o in  (out.argmax(dim=1) == label[test_index].reshape(-1)):\n",
    "    #         f.write(f\"{o}\\n\")\n",
    "    #     f.close()\n",
    "    return acc1, task_roc, task_pr, task_f1\n",
    "\n",
    "def main(tr, te, seed):\n",
    "    all_acc = []\n",
    "    all_roc = []\n",
    "    all_pr = []\n",
    "    all_f1 = []\n",
    "    for i in range(len(tr)):\n",
    "        f = open(f\"{i}foldtrain.txt\", \"w\", encoding=\"utf-8\")\n",
    "        train_index = tr[i]\n",
    "        for train_index_one in train_index:\n",
    "            f.write(f\"{train_index_one}\\n\")\n",
    "        test_index = te[i]\n",
    "        f = open(f\"{i}foldtest.txt\", \"w\", encoding=\"utf-8\")\n",
    "        for train_index_one in test_index:\n",
    "            f.write(f\"{train_index_one}\\n\")\n",
    "        #\n",
    "        # if not os.path.isdir(f\"{dir}\"):\n",
    "        #     os.makedirs(f\"{dir}\")\n",
    "\n",
    "        model = HyperGCNDTI(\n",
    "            num_protein=node_num[1],\n",
    "            num_drug=node_num[0],\n",
    "            num_hidden1=512,\n",
    "            num_hidden2=256,\n",
    "            num_out=128,\n",
    "        ).to(config['device'])\n",
    "            \n",
    "\n",
    "        # model.load_state_dict(torch.load(f\"{dir}/net{i}.pth\"))\n",
    "        optim = torch.optim.Adam(lr=lr, weight_decay=weight_decay, params=model.parameters())\n",
    "        best_acc = 0\n",
    "        best_pr = 0\n",
    "        best_roc = 0\n",
    "        best_f1 = 0\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            loss, train_acc, task1_roc, acc, task1_roc1, task1_pr, task1_f1 = train(model, optim, train_index, test_index, epoch, i)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "            if task1_pr > best_pr:\n",
    "                best_pr = task1_pr\n",
    "            if task1_roc1 > best_roc:\n",
    "                best_roc = task1_roc1\n",
    "            if task1_f1 > best_f1:\n",
    "                best_f1 = task1_f1\n",
    "        all_acc.append(best_acc)\n",
    "        all_roc.append(best_roc)\n",
    "        all_pr.append(best_pr)\n",
    "        all_f1.append(best_f1)\n",
    "\n",
    "    print(f'{\"Luo\"}, aver Acc is:{sum(all_acc) / len(all_acc):.4f},  Aver roc is:{sum(all_roc) / len(all_roc):.4f}, '\n",
    "    f'Aver Pr is:{sum(all_pr) / len(all_pr):.4f} ,Aver f1 is:{sum(all_f1) / len(all_f1):.4f}')\n",
    "\n",
    "\n",
    "train_indeces, test_indeces = get_cross(dtidata)\n",
    "main(train_indeces, test_indeces, config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check if model is on CUDA or CPU\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is on device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mnext\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example tensors\u001b[39;00m\n\u001b[1;32m      5\u001b[0m features_d \u001b[38;5;241m=\u001b[39m features_d\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check if model is on CUDA or CPU\n",
    "print(\"Model is on device:\", next(model.parameters()).device)\n",
    "\n",
    "# Example tensors\n",
    "features_d = features_d.to(config['device'])\n",
    "features_p = features_p.to(config['device'])\n",
    "\n",
    "# Check if tensors are on CUDA or CPU\n",
    "print(\"features_d is on device:\", features_d.device)\n",
    "print(\"features_p is on device:\", features_p.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_diease_eye = torch.cat((drug_disease, torch.eye(node_num[0])), dim=1)\n",
    "protein_disease_eye = torch.cat((protein_disease, torch.eye(node_num[0])), dim=1)\n",
    "\n",
    "HyGraph_Drug = generate_G_from_H(drug_protein_eye).to(config['device'])\n",
    "HyGraph_protein = generate_G_from_H(protein_drug_eye).to(config['device'])\n",
    "HyGraph_Drug_disease = generate_G_from_H(drug_diease_eye).to(config['device'])\n",
    "HyGraph_protein_disease = generate_G_from_H(protein_disease_eye).to(config['device'])\n",
    "\n",
    "\n",
    "drug_protein = drug_protein.to(config['device'])\n",
    "protein_drug = protein_drug.to(config['device'])\n",
    "\n",
    "\n",
    "HyGraph_Structure_DPP = HyGraph_Matrix_DPP_Structure(dtidata, node_num[0], node_num[1])\n",
    "HyGraph_Structure_DPP = HyGraph_Structure_DPP.to(config['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from layer import *\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from loaddata_utils import *\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperGCNDTI(nn.Module):\n",
    "    def __init__(self, num_protein, num_drug, num_hidden1, num_hidden2, num_out, dropout=0.5):  # 1512, 708, 512, 128\n",
    "        super(HyperGCNDTI, self).__init__()\n",
    "        self.DPP_GCN = DPP_GCN(num_hidden2, dropout)\n",
    "        self.MLP = MLP(num_hidden2)\n",
    "        self.alpha = 0.8\n",
    "\n",
    "        self.linear_d1 = nn.Linear(num_drug, num_hidden1)\n",
    "        self.linear_d2 = nn.Linear(num_hidden1, num_hidden2)\n",
    "        self.linear_d3 = nn.Linear(num_hidden2, num_out)\n",
    "\n",
    "        self.linear_p1 = nn.Linear(num_protein, num_hidden1)\n",
    "        self.linear_p2 = nn.Linear(num_hidden1, num_hidden2)\n",
    "        self.linear_p3 = nn.Linear(num_hidden2, num_out)\n",
    "\n",
    "        self.c = nn.Parameter(torch.Tensor((3), 1, 1))\n",
    "        self.d = nn.Parameter(torch.Tensor((3), 1, 1))\n",
    "\n",
    "        nn.init.constant_(self.c, 1)\n",
    "        nn.init.constant_(self.d, 1)\n",
    "\n",
    "    def SGC(self, feature, adj):\n",
    "\n",
    "        adj = adj + (torch.eye(adj.shape[0]).cuda()) * 2\n",
    "        deg = torch.sum(adj, dim=1)\n",
    "        deg[deg <= 1e-10] = 1\n",
    "        deg_inv = deg.pow(-0.5)\n",
    "        deg_inv = deg_inv * torch.eye(adj.shape[0]).type(torch.FloatTensor).cuda()\n",
    "        adj = torch.mm(deg_inv, adj)\n",
    "        adj = torch.mm(adj, deg_inv).type(torch.FloatTensor)\n",
    "\n",
    "        output = torch.mm(adj.cuda(), feature.cuda())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, node_num, drug_vec, protein_vec, protein_drug, drug_protein, HyGraph_Drug, HyGraph_protein, dateset_index, data, HyGraph_Structure_DPP, iftrain=True, d=None, p=None):\n",
    "\n",
    "        drug1 = F.relu(self.linear_d1(drug_vec))\n",
    "        drug2 = F.relu(self.linear_d2(drug1))\n",
    "        drug3 = F.relu(self.linear_d3(drug2))\n",
    "\n",
    "        protein1 = F.relu(self.linear_p1(protein_vec))\n",
    "        protein2 = F.relu(self.linear_p2(protein1))\n",
    "        protein3 = F.relu(self.linear_p3(protein2))\n",
    "        feature = torch.cat((drug3, protein3), dim=0)\n",
    "\n",
    "        HyGraph_ToGraph = torch.zeros((node_num[0] + node_num[1], node_num[0] + node_num[1])).cuda()\n",
    "        HyGraph_ToGraph[:node_num[0], node_num[0]:] = drug_protein\n",
    "        HyGraph_ToGraph[node_num[0]:, :node_num[0]] = protein_drug\n",
    "        HyGraph_ToGraph[:node_num[0], :node_num[0]] = HyGraph_Drug\n",
    "        HyGraph_ToGraph[node_num[0]:, node_num[0]:] = HyGraph_protein\n",
    "\n",
    "\n",
    "        adj = HyGraph_ToGraph\n",
    "        X_conv1 = self.SGC(feature, adj)\n",
    "        X_conv2 = self.SGC(X_conv1, adj)\n",
    "        X_conv3 = self.SGC(X_conv2, adj)\n",
    "        X_conv4 = self.SGC(X_conv3, adj)\n",
    "        X_conv5 = self.SGC(X_conv4, adj)\n",
    "        conv_sum = self.SGC(X_conv5, adj)\n",
    "        conv_sumx = self.SGC(conv_sum, adj)\n",
    "        drug_feature_ht = conv_sumx[:drug_vec.shape[0]]\n",
    "        protein_feature_ht = conv_sumx[drug_vec.shape[0]:]\n",
    "\n",
    "        if iftrain:\n",
    "            d, p = drug_feature_ht, protein_feature_ht\n",
    "\n",
    "        feature_dp = torch.cat((d[data[:, :1]], p[data[:, 1:2]]), dim=2)\n",
    "\n",
    "        feature_dp = feature_dp.squeeze(1)\n",
    "\n",
    "        f_edge, f_feature = constructure_knngraph(data, d, p)\n",
    "\n",
    "        z1, z2, z3, z4, z5, z6 = self.DPP_GCN(feature_dp, HyGraph_Structure_DPP, f_feature, f_edge)\n",
    "\n",
    "        att3 = F.softmax(self.c, dim=0)\n",
    "        feature_hg = torch.stack((z1, z2, z3), dim=0)\n",
    "        feature_hg = torch.sum((att3 * feature_hg), dim=0)\n",
    "\n",
    "        att4 = F.softmax(self.d, dim=0)\n",
    "        feature_knn = torch.stack((z4, z5, z6), dim=0)\n",
    "        feature_knn = torch.sum((att4 * feature_knn), dim=0)\n",
    "\n",
    "        feature_stack = torch.cat((feature_hg, feature_knn), dim=1)\n",
    "        pred = self.MLP(feature_stack[dateset_index])\n",
    "\n",
    "        if iftrain:\n",
    "            return pred, d, p\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =HyperGCNDTI(\n",
    "            num_protein=node_num[1],\n",
    "            num_drug=node_num[0],\n",
    "            num_hidden1=512,\n",
    "            num_hidden2=256,\n",
    "            num_out=128,\n",
    "        ).to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1512, 708])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_drug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([708, 708])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HyGraph_Drug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyGraph_ToGraph = torch.zeros((node_num[0] + node_num[1], node_num[0] + node_num[1])).cuda()\n",
    "HyGraph_ToGraph[:node_num[0], node_num[0]:] = drug_protein\n",
    "HyGraph_ToGraph[node_num[0]:, :node_num[0]] = protein_drug\n",
    "HyGraph_ToGraph[:node_num[0], :node_num[0]] = HyGraph_Drug\n",
    "HyGraph_ToGraph[node_num[0]:, node_num[0]:] = HyGraph_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2220, 2220])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HyGraph_ToGraph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3846, 3846])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HyGraph_Structure_DPP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "out, d, p = model(\n",
    "    node_num, \n",
    "    features_d, #drug_vec\n",
    "    features_p, #provec\n",
    "    protein_drug, \n",
    "    drug_protein, \n",
    "    HyGraph_Drug, \n",
    "    HyGraph_protein, \n",
    "    train_index, \n",
    "    dtidata, \n",
    "    HyGraph_Structure_DPP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    \"\"\"Compute accuracy based on predictions and true labels.\"\"\"\n",
    "    return (predictions.argmax(dim=1) == labels.reshape(-1).long()).sum(dtype=float) / torch.tensor(len(labels), dtype=float)\n",
    "\n",
    "def compute_loss(output, target, model, reg_loss_co):\n",
    "    \"\"\"Calculate the loss including L2 regularization.\"\"\"\n",
    "    classification_loss = F.nll_loss(output, target.reshape(-1).long())\n",
    "    reg = get_L2reg(model.parameters())\n",
    "    total_loss = classification_loss + reg_loss_co * reg\n",
    "    return total_loss\n",
    "\n",
    "def train(model, optim, train_index, test_index, epoch, fold, config):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to be trained.\n",
    "        optim: The optimizer used for training.\n",
    "        train_index: Indices for training samples.\n",
    "        test_index: Indices for testing samples.\n",
    "        epoch: Current epoch number.\n",
    "        fold: Current fold number for cross-validation.\n",
    "        config: A dictionary containing configuration parameters.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing training loss, accuracy, ROC, and test metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    model =HyperGCNDTI(\n",
    "                num_protein=node_num[1],\n",
    "                num_drug=node_num[0],\n",
    "                num_hidden1=512,\n",
    "                num_hidden2=256,\n",
    "                num_out=128,\n",
    "            ).to(config['device'])\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    out, d, p = model(\n",
    "        node_num, \n",
    "        features_d, \n",
    "        features_p, \n",
    "        protein_drug, \n",
    "        drug_protein, \n",
    "        HyGraph_Drug, \n",
    "        HyGraph_protein, \n",
    "        train_index, \n",
    "        dtidata, \n",
    "        HyGraph_Structure_DPP\n",
    "    )\n",
    "\n",
    "    # Compute training accuracy\n",
    "    train_acc = compute_accuracy(out, config['label'][train_index])\n",
    "    \n",
    "    # Compute ROC\n",
    "    task1_roc = get_roc(out, config['label'][train_index])\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(out, config['label'][train_index], model, config['reg_loss_co'])\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # Perform testing\n",
    "    te_acc, te_task1_roc1, te_task1_pr, te_task1_f1 = main_test(model, d, p, test_index, epoch, fold)\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"fold\": fold,\n",
    "        \"train_loss\": loss.item(),\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"train_roc\": task1_roc,\n",
    "        \"test_accuracy\": te_acc,\n",
    "        \"test_roc\": te_task1_roc1,\n",
    "        \"test_precision\": te_task1_pr,\n",
    "        \"test_f1\": te_task1_f1\n",
    "    })\n",
    "\n",
    "    # Return the results\n",
    "    return loss.item(), train_acc, task1_roc, te_acc, te_task1_roc1, te_task1_pr, te_task1_f1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
