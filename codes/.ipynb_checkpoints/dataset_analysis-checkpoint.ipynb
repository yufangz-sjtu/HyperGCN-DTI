{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/zyf/HyperGCN-DTI/codes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os \n",
    "from tqdm import trange\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from feature_fusion import ProteinFeatureExtractor,SMILESFeatureExtractor\n",
    "import esm\n",
    "print(os.getcwd())\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "#from utils import *\n",
    "import yaml\n",
    "from Structe_DPP_HyperGraph import HyGraph_Matrix_DPP_Structure\n",
    "import torch.nn.functional as F\n",
    "#import wandb\n",
    "from model import *\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from hypergraph_utils import generate_G_from_H\n",
    "from hypergraph_utils import construct_H_with_KNN\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_similarity_filter(data, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    根据化合物 SMILES 计算相似度并过滤\n",
    "    \n",
    "    参数:\n",
    "    - data: pd.DataFrame，必须包含 'SMILES' 列\n",
    "    - similarity_threshold: float，相似度阈值\n",
    "    \n",
    "    返回:\n",
    "    - pd.DataFrame：过滤后的 DataFrame\n",
    "    \"\"\"\n",
    "    # 将 SMILES 转换为 RDKit 分子对象\n",
    "    data['mol'] = data['SMILES'].map(lambda x: Chem.MolFromSmiles(x))\n",
    "    \n",
    "    # 计算分子指纹\n",
    "    data['fingerprint'] = data['mol'].map(lambda x: AllChem.GetMorganFingerprintAsBitVect(x, radius=2) if x else None)\n",
    "    \n",
    "    # 删除无法解析的分子\n",
    "    data = data[~data['fingerprint'].isna()].reset_index(drop=True)\n",
    "\n",
    "    # 计算相似度矩阵\n",
    "    fingerprints = list(data['fingerprint'])\n",
    "    num_molecules = len(fingerprints)\n",
    "    similarity_matrix = np.zeros((num_molecules, num_molecules))\n",
    "\n",
    "    for i in range(num_molecules):\n",
    "        for j in range(i + 1, num_molecules):\n",
    "            similarity = DataStructs.FingerprintSimilarity(fingerprints[i], fingerprints[j])\n",
    "            similarity_matrix[i, j] = similarity\n",
    "            similarity_matrix[j, i] = similarity\n",
    "\n",
    "    # 过滤相似度低于阈值的化合物\n",
    "    to_keep = []\n",
    "    for i in range(num_molecules):\n",
    "        if all(similarity_matrix[i, j] < similarity_threshold for j in range(i)):\n",
    "            to_keep.append(i)\n",
    "\n",
    "    filtered_data = data.iloc[to_keep].drop(['mol', 'fingerprint'], axis=1)\n",
    "    return filtered_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_bdb = pd.read_csv('../data/BindingDB/bdb_202501.csv',low_memory=False)\n",
    "old_bdb = pd.read_csv('../data/BindingDB/bdb_202310.csv',low_memory=False) \n",
    "old_bdb = old_bdb[old_bdb['Curation/DataSource'] == 'Curated from the literature by BindingDB']## 28456\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保列不为空\n",
    "new_bdb = new_bdb[new_bdb['IC50 (nM)'].notna()]\n",
    "\n",
    "# 遍历数据\n",
    "for i in range(len(new_bdb)):\n",
    "    try:\n",
    "        value = str(new_bdb.iloc[i, 5])  # 修改列索引为正确值\n",
    "        if '>' in value or '<' in value:\n",
    "            new_bdb.iloc[i, 5] = float(value[1:])  # 去掉符号后转换\n",
    "        else:\n",
    "            new_bdb.iloc[i, 5] = float(value)  # 正常转换\n",
    "    except ValueError:\n",
    "        print(f\"错误值：{new_bdb.iloc[i, 5]}\")  # 输出无法转换的值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bdb = new_bdb[(new_bdb['IC50 (nM)'] <= 100) | (new_bdb['IC50 (nM)'] >= 10000)] # 37349\n",
    "new_bdb ['label'] = 0\n",
    "new_bdb.loc[new_bdb['IC50 (nM)'] <= 100, 'label'] = 1\n",
    "new_bdb = new_bdb.rename(columns={'Ligand SMILES': 'SMILES','BindingDB Target Chain Sequence':'protein_sequence'})\n",
    "old_bdb = old_bdb.rename(columns={'mol': 'SMILES','BindingDB Target Chain Sequence':'protein_sequence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_count(BindingDB):\n",
    "    BindingDB_mol = list(BindingDB['SMILES'].unique())\n",
    "    BindingDB_mol = list(BindingDB['PubChem CID of Ligand'].unique())\n",
    "    print('numbers of BindingDB mol:',len(BindingDB_mol))\n",
    "    BindingDB_target = list(BindingDB['UniProt (SwissProt) Primary ID of Target Chain'].unique())\n",
    "    print('numbers of BindingDB targets:',len(BindingDB_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "def data_proprecessing(data):\n",
    "    data['protein_sequence']=data['protein_sequence'].map(lambda x: x.replace(' ',''))\n",
    "    data['protein_sequence']=data['protein_sequence'].map(lambda x: x.upper())\n",
    "    data['counts'] = data['SMILES'].map(lambda x: x.lower().count('c'))\n",
    "    #去掉无机物\n",
    "    \n",
    "    data = data[data.counts > 3 ].reset_index(drop = True)\n",
    "    data  = data.drop('counts',axis=1)\n",
    "    # 调用过滤函数\n",
    "    filtered_data = calculate_similarity_filter(data, similarity_threshold=0.7)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_count(new_bdb)\n",
    "stat_count(old_bdb)\n",
    "new_bdb = data_proprecessing(new_bdb)\n",
    "old_bdb = data_proprecessing(old_bdb)\n",
    "old_bdb.to_csv('../data/BindingDB/old_bdb.csv',index= False)\n",
    "new_bdb.to_csv('../data/BindingDB/new_bdb.csv',index= False)\n",
    "mol = pd.DataFrame(new_bdb['SMILES'].unique(),columns=['SMILES']) #24243\n",
    "target = pd.DataFrame(new_bdb['protein_sequence'].unique(),columns=['protein_sequence']) #1338\n",
    "mol.to_csv('../data/BindingDB/drug_smiles.csv',index=False)\n",
    "target.to_csv('../data/BindingDB/protein_seq.csv',index=False)\n",
    "extractor = ProteinFeatureExtractor(model_name='esm2_t6_8M_UR50D')\n",
    "extractor.extract_features_from_csv('../data/BindingDB/protein_seq.csv')\n",
    "extractor = SMILESFeatureExtractor(model_name='DeepChem/ChemBERTa-77M-MTR')\n",
    "extractor.process_and_save_features('../data/BindingDB/drug_smiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_bdb_dataset(new_bdb):\n",
    "    # Step 1: 获取唯一药物和蛋白质\n",
    "    unique_drugs = new_bdb['SMILES'].unique()\n",
    "    unique_proteins =new_bdb['protein_sequence'].unique()\n",
    "\n",
    "    # Step 2: 为药物和蛋白质分配索引\n",
    "    drug_to_index = {drug: i for i, drug in enumerate(unique_drugs)}\n",
    "    protein_to_index = {protein: i for i, protein in enumerate(unique_proteins)}\n",
    "\n",
    "    # Step 3: 替换药物和蛋白质为索引\n",
    "    new_bdb['drug_index'] = new_bdb['SMILES'].map(drug_to_index)\n",
    "    new_bdb['protein_index'] = new_bdb['protein_sequence'].map(protein_to_index)\n",
    "    # Step 1: 获取唯一药物和蛋白质\n",
    "    unique_drugs = new_bdb['SMILES'].unique()\n",
    "    unique_proteins =new_bdb['protein_sequence'].unique()\n",
    "\n",
    "    # Step 2: 为药物和蛋白质分配索引\n",
    "    drug_to_index = {drug: i for i, drug in enumerate(unique_drugs)}\n",
    "    protein_to_index = {protein: i for i, protein in enumerate(unique_proteins)}\n",
    "\n",
    "    # Step 3: 替换药物和蛋白质为索引\n",
    "    new_bdb['drug_index'] = new_bdb['SMILES'].map(drug_to_index)\n",
    "    new_bdb['protein_index'] = new_bdb['protein_sequence'].map(protein_to_index)\n",
    "    \n",
    "    num_drug = len(unique_drugs)\n",
    "    num_protein = len(unique_proteins)\n",
    "    interaction_matrix = np.zeros((num_drug, num_protein), dtype=int)\n",
    "    for _, row in new_bdb.iterrows():\n",
    "        interaction_matrix[row['drug_index'], row['protein_index']] = row['label']\n",
    "    \n",
    "    # 将数据集写入文件\n",
    "    data_set = np.array(new_bdb[['drug_index','protein_index','label']])\n",
    "    with open(\"../data/BindingDB/dti_index.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in data_set:\n",
    "            f.write(f\"{i[0]}\\t{i[1]}\\t{i[2]}\\n\")\n",
    "\n",
    "\n",
    "    col1 = data_set[:, 0]\n",
    "    col2 = data_set[:, 1]\n",
    "\n",
    "\n",
    "    col1_dict = {}\n",
    "    col2_dict = {}\n",
    "    for i, val in enumerate(col1):\n",
    "        col1_dict.setdefault(val, []).append(i)\n",
    "    for i, val in enumerate(col2):\n",
    "        col2_dict.setdefault(val, []).append(i)\n",
    "\n",
    "\n",
    "    rows, cols = [], []\n",
    "    for indices in col1_dict.values():\n",
    "        for i in indices:\n",
    "            for j in indices:\n",
    "                if i <= j:\n",
    "                    rows.append(i)\n",
    "                    cols.append(j)\n",
    "\n",
    "    for indices in col2_dict.values():\n",
    "        for i in indices:\n",
    "            for j in indices:\n",
    "                if i <= j:\n",
    "                    rows.append(i)\n",
    "                    cols.append(j)\n",
    "\n",
    "    # 去重\n",
    "    edges = set(zip(rows, cols))\n",
    "\n",
    "    # 写入文件\n",
    "    with open(\"../data/BindingDB/dtiedge.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, j in edges:\n",
    "            f.write(f\"{i}\\t{j}\\n\")\n",
    "\n",
    "    node_num = [num_drug, num_protein]\n",
    "    drug_protein_tensor = torch.Tensor(interaction_matrix)\n",
    "    protein_drug_tensor = drug_protein_tensor.t()\n",
    "    return node_num, drug_protein_tensor, protein_drug_tensor, data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_index(new_bdb,old_bdb):\n",
    "    # 创建 drug-protein pairs 列\n",
    "    new_bdb['drug_protein_pair'] = new_bdb['PubChem CID of Ligand'].astype(str)+ '_' + new_bdb['UniProt (SwissProt) Primary ID of Target Chain']\n",
    "    old_bdb['drug_protein_pair'] = old_bdb['PubChem CID of Ligand'].astype(str)+ '_' + old_bdb['UniProt (SwissProt) Primary ID of Target Chain']\n",
    "\n",
    "    #标注测试集\n",
    "    # 1. 共有的 drug-protein pairs\n",
    "    common_pairs = new_bdb[new_bdb['drug_protein_pair'].isin(old_bdb['drug_protein_pair'])]\n",
    "    print(\"common drug-protein pairs:\",len(common_pairs))\n",
    "\n",
    "    # 2. old drug-new protein pairs\n",
    "    new_bdb_without_common = new_bdb[~new_bdb['drug_protein_pair'].isin(common_pairs['drug_protein_pair'])]\n",
    "    old_drugs = old_bdb['PubChem CID of Ligand'].unique()\n",
    "    new_proteins = new_bdb['UniProt (SwissProt) Primary ID of Target Chain'].unique()\n",
    "    old_drug_new_protein_pairs = new_bdb_without_common[new_bdb_without_common['PubChem CID of Ligand'].isin(old_drugs) & new_bdb_without_common['UniProt (SwissProt) Primary ID of Target Chain'].isin(new_proteins)]\n",
    "    print(\"old drug-new protein pairs:\",len(old_drug_new_protein_pairs))\n",
    "    print()\n",
    "\n",
    "    # 3. new drug-old protein pairs\n",
    "    new_drugs = new_bdb['PubChem CID of Ligand'].unique()\n",
    "    old_proteins = old_bdb['UniProt (SwissProt) Primary ID of Target Chain'].unique()\n",
    "    new_drug_old_protein_pairs = new_bdb_without_common[new_bdb_without_common['PubChem CID of Ligand'].isin(new_drugs) & new_bdb_without_common['UniProt (SwissProt) Primary ID of Target Chain'].isin(old_proteins)]\n",
    "    print(\"new drug-old protein pairs:\",len(new_drug_old_protein_pairs))\n",
    "    # 4. new drug-new protein pairs\n",
    "    new_drug_new_protein_pairs =new_bdb_without_common[~new_bdb['PubChem CID of Ligand'].isin(old_bdb['PubChem CID of Ligand']) & \n",
    "                                                        ~new_bdb_without_common['UniProt (SwissProt) Primary ID of Target Chain'].isin(old_bdb['UniProt (SwissProt) Primary ID of Target Chain'])]\n",
    "    print(\"new drug-new protein pairs:\",len(new_drug_new_protein_pairs))\n",
    "    \n",
    "    train_indeces = np.array(common_pairs.index)\n",
    "    test_tp_indeces = np.array(old_drug_new_protein_pairs.index)\n",
    "    test_td_indeces = np.array(new_drug_old_protein_pairs.index)\n",
    "    test_tn_indeces = np.array(new_drug_new_protein_pairs.index)\n",
    "    return train_indeces,test_tp_indeces,test_td_indeces,test_tn_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "#config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#config['device'] = \"cpu\"\n",
    "\n",
    "setup(config['seed'])\n",
    "\n",
    "\n",
    "reg_loss_co = 0.0002\n",
    "fold = 0\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common drug-protein pairs: 6881\n",
      "old drug-new protein pairs: 40\n",
      "\n",
      "new drug-old protein pairs: 2631\n",
      "new drug-new protein pairs: 108\n",
      "load LLM features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'common drug-protein pairs: 6881\\nold drug-new protein pairs: 40\\n\\nnew drug-old protein pairs: 2631\\nnew drug-new protein pairs: 108'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''1:HyperDrug & HyperProtein features\n",
    "    2:sequence feature from pre-trained LLM model\n",
    "3:HyperDrug-Disease & HyperProtein-Disease features\n",
    "    input:list,contains types of features ''' \n",
    "\n",
    "'为了后续图架构，保证使用HyperDrug和HyperProtein特征' \n",
    "new_bdb = pd.read_csv('../data/BindingDB/new_bdb.csv')\n",
    "old_bdb = pd.read_csv('../data/BindingDB/old_bdb.csv')\n",
    "train_indeces,test_tp_indeces,test_td_indeces,test_tn_indeces = get_train_test_index(new_bdb,old_bdb)\n",
    "node_num, drug_protein, protein_drug, dtidata = construct_bdb_dataset(new_bdb)\n",
    "\n",
    "drug_protein_eye = torch.cat((drug_protein, torch.eye(node_num[0])), dim=1)\n",
    "protein_drug_eye = torch.cat((protein_drug, torch.eye(node_num[1])), dim=1)\n",
    "HyGraph_Drug = generate_G_from_H(drug_protein_eye).to(config['device'])\n",
    "HyGraph_protein = generate_G_from_H(protein_drug_eye).to(config['device'])\n",
    "#print('HyGraph_Drug,HyGraph_protein:',HyGraph_Drug.shape,HyGraph_protein.shape)\n",
    "\n",
    "if  config['feature_list'] ==[1,2]:\n",
    "    \n",
    "    hd = pd.read_csv(f'/data/zyf/HyperGCN-DTI/data/BindingDB/drug_smiles.csv')\n",
    "    hp = pd.read_csv(f'/data/zyf/HyperGCN-DTI/data/BindingDB/protein_seq.csv')\n",
    "\n",
    "    features_d = torch.tensor(hd.iloc[:,1:].values,dtype=torch.float32).to(config['device'])\n",
    "    features_p = torch.tensor(hp.iloc[:,1:].values,dtype=torch.float32).to(config['device'])\n",
    "    print('load LLM features')\n",
    "\n",
    "'''common drug-protein pairs: 6881\n",
    "old drug-new protein pairs: 40\n",
    "\n",
    "new drug-old protein pairs: 2631\n",
    "new drug-new protein pairs: 108'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_label = torch.tensor(dtidata[:, 2:3]).to(config['device'])\n",
    "drug_protein = drug_protein.to(config['device'])\n",
    "protein_drug = protein_drug.to(config['device'])\n",
    "HyGraph_Structure_DPP = HyGraph_Matrix_DPP_Structure(dtidata, node_num[0], node_num[1],'BindingDB' )\n",
    "HyGraph_Structure_DPP = HyGraph_Structure_DPP.to(config['device'])\n",
    "data = dtidata\n",
    "label = dti_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9623, 832] torch.Size([9623, 832]) torch.Size([832, 9623]) (9623, 3) torch.Size([9623, 384]) torch.Size([832, 320]) torch.Size([9623, 9623]) torch.Size([832, 832])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'numpy.ndarray'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(node_num,drug_protein.shape,protein_drug.shape,dtidata.shape,features_d.shape,features_p.shape,HyGraph_Drug.shape,HyGraph_protein.shape)\n",
    "print(type(drug_protein),type(protein_drug),type(dtidata),type(features_d),type(features_p),type(HyGraph_Drug),type(HyGraph_protein))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6881 40 2631 108 9623\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indeces),len(test_tp_indeces),len(test_td_indeces),len(test_tn_indeces),len(new_bdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, optim, train_index, epoch):\n",
    "    model.train()\n",
    "    out, d, p = model(node_num, features_d, features_p, protein_drug, drug_protein, HyGraph_Drug, HyGraph_protein, train_index, data, HyGraph_Structure_DPP)\n",
    "    tr_acc = (out.argmax(dim=1) == label[train_index].reshape(-1).long()).sum(dtype=float) / torch.tensor(len(train_index), dtype=float)\n",
    "    tr_task1_roc = get_roc(out, label[train_index])\n",
    "    reg = get_L2reg(model.parameters())\n",
    "    loss = F.nll_loss(out, label[train_index].reshape(-1).long()) + reg_loss_co * reg\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    tr_acc, tr_task1_roc1, tr_task1_pr, tr_task_precision, tr_task_recall, tr_task1_f1 = main_test(model, d, p, train_indeces)\n",
    "    return loss.item(), tr_acc, tr_task1_roc1, tr_task1_pr, tr_task_precision, tr_task_recall, tr_task1_f1, d, p\n",
    "\n",
    "\n",
    "\n",
    "def main_test(model, d, p, test_index):\n",
    "    model.eval()\n",
    "    out = model(node_num, features_d, features_p, protein_drug, drug_protein, HyGraph_Drug, HyGraph_protein, test_index, data, HyGraph_Structure_DPP, iftrain=False, d=d, p=p)\n",
    "    acc1 = (out.argmax(dim=1) == label[test_index].reshape(-1).long()).sum(dtype=float) / torch.tensor(len(test_index), dtype=float)\n",
    "    task_roc = get_roc(out, label[test_index])\n",
    "    task_precision,task_recall,task_pr = get_pr(out, label[test_index])\n",
    "    task_f1 = get_f1score(out, label[test_index])\n",
    "    return acc1, task_roc, task_pr, task_precision,task_recall,task_f1\n",
    "\n",
    "def main(train_index, test_tp_indeces,test_td_indeces,test_tn_indeces, seed):\n",
    "\n",
    "    model = HyperGCNDTI(\n",
    "        num_protein=node_num[1],\n",
    "        num_drug=node_num[0],\n",
    "        num_hidden1=config['in_size'],\n",
    "        num_hidden2=config['hidden_size'],\n",
    "        num_out=config['out_size'],\n",
    "        feature_list= config['feature_list']\n",
    "    ).to(config['device'])\n",
    "        \n",
    "    # model.load_state_dict(torch.load(f\"{dir}/net{i}.pth\"))\n",
    "    optim = torch.optim.Adam(lr=config['lr'], weight_decay= float(config['weight_decay']), params=model.parameters())\n",
    "    best_roc =0\n",
    "    best_results = []\n",
    "    model_path = os.path.join(config['save_dir'], f\"{config['feature_list']}_dataset_BindingDB_best_model_roc.pth\")\n",
    "\n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        loss, acc, task1_roc1, task1_pr, task1_precision, task1_recall, task1_f1, d, p = train(model, optim, train_index, epoch)\n",
    "        if task1_roc1 > best_roc:\n",
    "            best_roc = task1_roc1\n",
    "            best_model_state = model.state_dict()  # Update the best model state\n",
    "            torch.save(best_model_state, model_path)\n",
    "            best_results = acc, task1_roc1, task1_pr, task1_precision, task1_recall, task1_f1\n",
    "            best_results = list(tuple(f\"{value:.4f}\" for value in best_results))\n",
    "\n",
    "    # 加载最佳模型并测试\n",
    "    print(\"Training finished!\", best_results)\n",
    "    best_model_state = torch.load(model_path)\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "\n",
    "    # 测试结果\n",
    "    results = {}\n",
    "\n",
    "    # 训练集结果\n",
    "    tr_acc, tr_task1_roc1, tr_task1_pr, tr_task_precision, tr_task_recall, tr_task1_f1 = main_test(model, d, p, train_index)\n",
    "    results[\"train\"] = {\n",
    "        \"accuracy\": tr_acc,\n",
    "        \"roc\": tr_task1_roc1,\n",
    "        \"pr\": tr_task1_pr,\n",
    "        \"precision\": tr_task_precision,\n",
    "        \"recall\": tr_task_recall,\n",
    "        \"f1\": tr_task1_f1\n",
    "    }\n",
    "\n",
    "    # 测试集 tp\n",
    "    tp_acc, tp_task1_roc1, tp_task1_pr, tp_task_precision, tp_task_recall, tp_task1_f1 = main_test(model, d, p, test_tp_indeces)\n",
    "    results[\"test_tp\"] = {\n",
    "        \"accuracy\": tp_acc,\n",
    "        \"roc\": tp_task1_roc1,\n",
    "        \"pr\": tp_task1_pr,\n",
    "        \"precision\": tp_task_precision,\n",
    "        \"recall\": tp_task_recall,\n",
    "        \"f1\": tp_task1_f1\n",
    "    }\n",
    "\n",
    "    # 测试集 td\n",
    "    td_acc, td_task1_roc1, td_task1_pr, td_task_precision, td_task_recall, td_task1_f1 = main_test(model, d, p, test_td_indeces)\n",
    "    results[\"test_td\"] = {\n",
    "        \"accuracy\": td_acc,\n",
    "        \"roc\": td_task1_roc1,\n",
    "        \"pr\": td_task1_pr,\n",
    "        \"precision\": td_task_precision,\n",
    "        \"recall\": td_task_recall,\n",
    "        \"f1\": td_task1_f1\n",
    "    }\n",
    "\n",
    "    # 测试集 tn\n",
    "    tn_acc, tn_task1_roc1, tn_task1_pr, tn_task_precision, tn_task_recall, tn_task1_f1 = main_test(model, d, p, test_tn_indeces)\n",
    "    results[\"test_tn\"] = {\n",
    "        \"accuracy\": tn_acc,\n",
    "        \"roc\": tn_task1_roc1,\n",
    "        \"pr\": tn_task1_pr,\n",
    "        \"precision\": tn_task_precision,\n",
    "        \"recall\": tn_task_recall,\n",
    "        \"f1\": tn_task1_f1\n",
    "    }\n",
    "\n",
    "    # 保存结果\n",
    "    df_results = pd.DataFrame(results).T\n",
    "    df_results.index.name = \"dataset\"\n",
    "    df_results.to_csv(os.path.join(config['results_dir'], f\"BindingDB_{config['feature_list']}_results.csv\"), index=False)\n",
    "    return df_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [20:07<00:00, 12.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished! ['0.9943', '1.0000', '1.0000', '1.0000', '0.9854', '0.9927']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc</th>\n",
       "      <th>pr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>tensor(0.9951, device='cuda:0', dtype=torch.fl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987313</td>\n",
       "      <td>0.993616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_tp</th>\n",
       "      <td>tensor(1., device='cuda:0', dtype=torch.float64)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_td</th>\n",
       "      <td>tensor(0.9962, device='cuda:0', dtype=torch.fl...</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.999824</td>\n",
       "      <td>0.993506</td>\n",
       "      <td>0.990291</td>\n",
       "      <td>0.991896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_tn</th>\n",
       "      <td>tensor(1., device='cuda:0', dtype=torch.float64)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  accuracy       roc  \\\n",
       "dataset                                                                \n",
       "train    tensor(0.9951, device='cuda:0', dtype=torch.fl...       1.0   \n",
       "test_tp   tensor(1., device='cuda:0', dtype=torch.float64)       1.0   \n",
       "test_td  tensor(0.9962, device='cuda:0', dtype=torch.fl...  0.999946   \n",
       "test_tn   tensor(1., device='cuda:0', dtype=torch.float64)       1.0   \n",
       "\n",
       "               pr precision    recall        f1  \n",
       "dataset                                          \n",
       "train         1.0       1.0  0.987313  0.993616  \n",
       "test_tp       1.0       1.0       1.0       1.0  \n",
       "test_td  0.999824  0.993506  0.990291  0.991896  \n",
       "test_tn       1.0       1.0       1.0       1.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(train_indeces, test_tp_indeces,test_td_indeces,test_tn_indeces, config['seed'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
