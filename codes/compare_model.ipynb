{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import warnings\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from layer import *\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#config['device'] = \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "setup(config['seed'])\n",
    "\n",
    "\n",
    "reg_loss_co = 0.0002\n",
    "fold = 0\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN—Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, 128)\n",
    "        self.gc2 = GraphConvolution(128, 128)\n",
    "        self.gc3 = GraphConvolution(128, 128)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = x.to(device)\n",
    "        adj = adj.to(device)\n",
    "        x11 = F.relu(self.gc1(x, adj), inplace=True)  # 试下tanh\n",
    "        x1 = F.dropout(x11, self.dropout)\n",
    "        x22 = self.gc2(x1, adj)\n",
    "        x2 = F.dropout(x22, self.dropout)\n",
    "        x3 = self.gc3(x2, adj)\n",
    " \n",
    "        return x11, x22, x3\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nfeat):\n",
    "        super(MLP, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(nfeat, 64, bias=False).apply(init),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 16, bias=False).apply(init),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(16, 2, bias=False),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.MLP(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knngraph(dateset, feature, aug=False):\n",
    "\n",
    "    fedge = np.array(generate_knn(feature.cpu().detach().numpy()))\n",
    "    fedge = load_graph(np.array(fedge), dateset.shape[0])\n",
    "    edg = torch.Tensor.to_dense(fedge)\n",
    "    edge = edg.numpy()\n",
    "    return fedge, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_protein, num_drug, num_hidden1, num_hidden2, num_out, dropout=0.5,feature_list = [1,2,3]):\n",
    "        super(KNN_model, self).__init__()\n",
    "        self.GCN  = GCN(num_hidden2, dropout)\n",
    "        self.MLP = MLP(num_hidden2)\n",
    "        self.alpha = 0.8\n",
    "        self.d = nn.Parameter(torch.Tensor((3), 1, 1))\n",
    "    def SGC(self, feature, adj):\n",
    "        if adj.is_sparse:\n",
    "            adj = adj.to_dense()\n",
    "        adj = adj + (torch.eye(adj.shape[0]).cuda()) * 2\n",
    "        deg = torch.sum(adj, dim=1)\n",
    "        deg[deg <= 1e-10] = 1\n",
    "        deg_inv = deg.pow(-0.5)\n",
    "        deg_inv = deg_inv * torch.eye(adj.shape[0]).type(torch.FloatTensor).cuda()\n",
    "        adj = torch.mm(deg_inv, adj)\n",
    "        adj = torch.mm(adj, deg_inv).type(torch.FloatTensor)\n",
    "\n",
    "        output = torch.mm(adj.cuda(), feature.cuda())\n",
    "\n",
    "        return output\n",
    "    def forward(self,drug_vec, feature, fedge,dateset_index,  iftrain=True, d=None, p=None):\n",
    "\n",
    "        adj =  fedge\n",
    "     \n",
    "        X_conv1 = self.SGC(feature, adj)\n",
    "        X_conv2 = self.SGC(X_conv1, adj)\n",
    "        X_conv3 = self.SGC(X_conv2, adj)\n",
    "        X_conv4 = self.SGC(X_conv3, adj)\n",
    "        X_conv5 = self.SGC(X_conv4, adj)\n",
    "        conv_sum = self.SGC(X_conv5, adj)\n",
    "        conv_sumx = self.SGC(conv_sum, adj)\n",
    "        drug_feature_ht = conv_sumx[:drug_vec.shape[0]]\n",
    "        protein_feature_ht = conv_sumx[drug_vec.shape[0]:]\n",
    "        if iftrain:\n",
    "            d, p = drug_feature_ht, protein_feature_ht\n",
    "        z4, z5, z6 = self.GCN(feature, fedge)\n",
    "        att4 = F.softmax(self.d, dim=0)\n",
    "        feature_knn = torch.stack((z4, z5, z6), dim=0)\n",
    "        feature_knn = torch.sum((att4 * feature_knn), dim=0)\n",
    "        pred = self.MLP(feature_knn[dateset_index])\n",
    "        #pred =  self._MLP (feature_knn[dateset_index])\n",
    "        if iftrain:\n",
    "            return pred, d, p\n",
    "        return pred\n",
    "\n",
    "\n",
    "def init(i):\n",
    "    if isinstance(i, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(i.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_test(model, d, p, test_index, epoch, fold):\n",
    "    model.eval()\n",
    "    out = model( features_d, feature, fedge, test_index)\n",
    "\n",
    "    acc1 = (out.argmax(dim=1) == label[test_index].reshape(-1).long()).sum(dtype=float) / torch.tensor(len(test_index), dtype=float)\n",
    "    task_roc = get_roc(out, label[test_index])\n",
    "    task_precision,task_recall,task_pr = get_pr(out, label[test_index])\n",
    "    task_f1 = get_f1score(out, label[test_index])\n",
    "    # if epoch == 799:\n",
    "    #     f = open(f\"{fold}out.txt\",\"w\",encoding=\"utf-8\")\n",
    "    #     for o in  (out.argmax(dim=1) == label[test_index].reshape(-1)):\n",
    "    #         f.write(f\"{o}\\n\")\n",
    "    #     f.close()\n",
    "    return acc1, task_roc, task_pr, task_precision,task_recall,task_f1\n",
    "\n",
    "def train(model, optim, train_index, test_index, epoch, fold):\n",
    "    model.train()\n",
    "    out, d, p = model(features_d,feature, fedge, train_index)\n",
    "    #print(out.shape,d.shape,p.shape)\n",
    "    train_acc = (out.argmax(dim=1) == label[train_index].reshape(-1).long()).sum(dtype=float) / torch.tensor(len(train_index), dtype=float)\n",
    "    task1_roc = get_roc(out, label[train_index])\n",
    "    reg = get_L2reg(model.parameters())\n",
    "    loss = F.nll_loss(out, label[train_index].reshape(-1).long()) + reg_loss_co * reg\n",
    "    #print(train_acc,loss)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    te_acc, te_task1_roc1, te_task1_pr, te_task_precision,te_task_recall,te_task1_f1 = main_test(model, d, p, test_index, epoch, fold)\n",
    "\n",
    "    return loss.item(), train_acc, task1_roc, te_acc, te_task1_roc1, te_task1_pr, te_task_precision,te_task_recall,te_task1_f1\n",
    "\n",
    "\n",
    "def main(tr, te, seed):\n",
    "        results = []\n",
    "        for i in range(len(tr)):\n",
    "            f = open( os.path.join(config['results_dir'],f\"{name}_{config['feature_list']}_{i}foldtrain.txt\"), \"w\", encoding=\"utf-8\")\n",
    "            train_index = tr[i]\n",
    "            for train_index_one in train_index:\n",
    "                f.write(f\"{train_index_one}\\n\")\n",
    "            test_index = te[i]\n",
    "            f = open( os.path.join(config['results_dir'],f\"{name}_{config['feature_list']}_{i}foldtest.txt\"), \"w\", encoding=\"utf-8\")\n",
    "            for train_index_one in test_index:\n",
    "                f.write(f\"{train_index_one}\\n\")\n",
    "            #\n",
    "            # if not os.path.isdir(f\"{dir}\"):\n",
    "            #     os.makedirs(f\"{dir}\")\n",
    "                \n",
    "            # model.load_state_dict(torch.load(f\"{dir}/net{i}.pth\"))\n",
    "            optim = torch.optim.Adam(lr=config['lr'], weight_decay= float(config['weight_decay']), params=model.parameters())\n",
    "            best_roc =0\n",
    "            best_results = []\n",
    "            for epoch in tqdm(range(config['epochs'])):\n",
    "                loss, train_acc, task1_roc, acc, task1_roc1, task1_pr,task1_precision,task1_recall, task1_f1 = train(model, optim, train_index, test_index, epoch, i)\n",
    "                if task1_roc1 > best_roc:\n",
    "                    best_roc = task1_roc1\n",
    "                    best_model_state = model.state_dict()  # Update the best model state\n",
    "                    torch.save(best_model_state, os.path.join(config['save_dir'], f\"{config['feature_list']}_dataset_{name}_best_model_fold_{i}_roc.pth\"))\n",
    "                    best_results = acc, task1_roc1, task1_pr,task1_precision,task1_recall, task1_f1\n",
    "                    best_results = list(tuple(f\"{value:.4f}\" for value in best_results))\n",
    "\n",
    "            results.append(best_results)\n",
    "\n",
    "            print(f'{i} fold cv results:', f'dataset {name}, Acc is:{best_results[0]},  AUROC is:{best_results[1]}, AUPr is:{best_results[2]},\\\n",
    "Precision is:{best_results[3]} , recall is :{best_results[4]},f1 is:{best_results[5]}')\n",
    "        df_results = pd.DataFrame(results, columns=['Accuracy', 'AUROC', 'AUPr',\"Precision\",\"recall\",\"f1\"])\n",
    "        print(f'5-fold cv averaged results:', f'dataset {name}')\n",
    "        print( df_results.mean())\n",
    "        df_results.to_csv(os.path.join(config['results_dir'],f\"{name}_{config['feature_list']}_CV_resluts.csv\"),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load LLM features\n",
      "torch.Size([3846, 3846]) torch.Size([3846, 704])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ** On entry to SGEMM  parameter number 10 had an illegal value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 31\u001b[0m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m KNN_model(\n\u001b[1;32m     24\u001b[0m                     num_protein\u001b[38;5;241m=\u001b[39mnode_num[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     25\u001b[0m                     num_drug\u001b[38;5;241m=\u001b[39mnode_num[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m                     num_out\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     29\u001b[0m                     feature_list\u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_list\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     30\u001b[0m train_indeces, test_indeces \u001b[38;5;241m=\u001b[39m get_cross(dtidata)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_indeces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indeces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(tr, te, seed)\u001b[0m\n\u001b[1;32m     52\u001b[0m best_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m---> 54\u001b[0m     loss, train_acc, task1_roc, acc, task1_roc1, task1_pr,task1_precision,task1_recall, task1_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task1_roc1 \u001b[38;5;241m>\u001b[39m best_roc:\n\u001b[1;32m     56\u001b[0m         best_roc \u001b[38;5;241m=\u001b[39m task1_roc1\n",
      "Cell \u001b[0;32mIn[36], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optim, train_index, test_index, epoch, fold)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, optim, train_index, test_index, epoch, fold):\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 18\u001b[0m     out, d, p \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfedge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#print(out.shape,d.shape,p.shape)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m (out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m label[train_index]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong())\u001b[38;5;241m.\u001b[39msum(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlen\u001b[39m(train_index), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[35], line 42\u001b[0m, in \u001b[0;36mKNN_model.forward\u001b[0;34m(self, drug_vec, feature, fedge, dateset_index, iftrain, d, p)\u001b[0m\n\u001b[1;32m     40\u001b[0m feature_knn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((z4, z5, z6), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     41\u001b[0m feature_knn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((att4 \u001b[38;5;241m*\u001b[39m feature_knn), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_knn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdateset_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#pred =  self._MLP (feature_knn[dateset_index])\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iftrain:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[33], line 32\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 32\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "for name in ['Luo',\"Es\",\"GPCRs\",\"ICs\",\"NRs\",'Zheng']: \n",
    "    node_num, drug_protein, protein_drug, dtidata,features_d,features_p,HyGraph_Drug,HyGraph_protein = load_feature(name,config['feature_list'])\n",
    "    #print(dtidata)\n",
    "    #dti_label = F.one_hot(torch.tensor(dtidata[:, 2:3]), num_classes=2)\n",
    "    #dti_label = torch.squeeze(dti_label, dim=1)\n",
    "    \n",
    "    dti_label = torch.tensor(dtidata[:, 2:3])\n",
    "    dti_label = dti_label.squeeze() \n",
    "    #print(dti_label)\n",
    "    indices_features1 = dtidata[:, 0]  # 提取第一个 Tensor 的行索引\n",
    "    indices_features2 = dtidata[:, 1]  # 提取第二个 Tensor 的行索引\n",
    "    selected_features1 = features_d[indices_features1]\n",
    "    selected_features2 = features_p[indices_features2]\n",
    "    features = torch.cat((selected_features1, selected_features2), dim=1)\n",
    "    features, dti_label = shuffle(features, dti_label, random_state=42)\n",
    "    \n",
    "    fedge, feature = knngraph(dtidata, features)\n",
    "    fedge =  fedge.to(config['device'])\n",
    "    feature = feature.to(config['device'])\n",
    "    print(fedge.shape,feature.shape)\n",
    "    label = dti_label\n",
    "    data = dtidata\n",
    "    model = KNN_model(\n",
    "                        num_protein=node_num[1],\n",
    "                        num_drug=node_num[0],\n",
    "                        num_hidden1=config['in_size'],\n",
    "                        num_hidden2=config['hidden_size'],\n",
    "                        num_out=config['out_size'],\n",
    "                        feature_list= config['feature_list']).to(config['device'])\n",
    "    train_indeces, test_indeces = get_cross(dtidata)\n",
    "    main(train_indeces, test_indeces, config['seed'])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
